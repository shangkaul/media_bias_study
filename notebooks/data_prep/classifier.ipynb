{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457460e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      1.00      0.80        26\n",
      "           1       1.00      0.07      0.13        14\n",
      "\n",
      "    accuracy                           0.68        40\n",
      "   macro avg       0.83      0.54      0.47        40\n",
      "weighted avg       0.78      0.68      0.57        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load the labeled dataset\n",
    "with open('training_articles.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Parse data: use 'text' as input and 'relevant' as label (convert to int if needed)\n",
    "df = pd.DataFrame([{'text': item.get('text', ''), 'label': 1 if str(item.get('relevant', '')).lower() == 'true' else 0} for item in data])\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a logistic regression classifier\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_vec, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "y_pred = clf.predict(X_test_vec)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03c8375",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/raid1/MSCs/AY2425/skaul/miniconda3/envs/scraper/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/199679458.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:11, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.644800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.527300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.292400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1 = 0.7500\n",
      "\n",
      "### Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/199679458.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.220500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.264900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1 = 0.8889\n",
      "\n",
      "### Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/199679458.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.744500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.483100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.368300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.292300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1 = 0.5600\n",
      "\n",
      "### Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/199679458.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.631300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.458400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.198400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1 = 0.9091\n",
      "\n",
      "### Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/199679458.py:98: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.645800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.429600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.330100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1 = 0.7619\n",
      "\n",
      "Cross-validated F1 scores: [0.75, 0.8888888888888888, 0.56, 0.9090909090909091, 0.7619047619047619]\n",
      "Mean F1: 0.7740\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load & prepare data once\n",
    "with open(\"training_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"text\": item.get(\"text\", \"\"),\n",
    "        \"label\": 1 if str(item.get(\"relevant\", \"\")).lower() == \"true\" else 0,\n",
    "    }\n",
    "    for item in data\n",
    "])\n",
    "\n",
    "# 2. Encode labels\n",
    "le = LabelEncoder()\n",
    "df[\"label_enc\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "# 3. Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_texts(texts, max_length=256):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# 4. Metrics fn\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    labels = p.label_ids\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1\":       f1_score(labels, preds, average=\"binary\"),\n",
    "    }\n",
    "\n",
    "# 5. Prepare cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(df[\"text\"], df[\"label_enc\"]), 1):\n",
    "    print(f\"\\n### Fold {fold}\")\n",
    "\n",
    "    # Split into texts & labels\n",
    "    train_texts = df.loc[train_idx, \"text\"]\n",
    "    train_labels = df.loc[train_idx, \"label_enc\"]\n",
    "    val_texts   = df.loc[val_idx,   \"text\"]\n",
    "    val_labels   = df.loc[val_idx,   \"label_enc\"]\n",
    "\n",
    "    # Tokenize\n",
    "    train_enc = tokenize_texts(train_texts)\n",
    "    val_enc   = tokenize_texts(val_texts)\n",
    "\n",
    "    # Dataset wrapper\n",
    "    class NewsDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, encodings, labels):\n",
    "            self.encodings = encodings\n",
    "            self.labels    = torch.tensor(labels.values if hasattr(labels, \"values\") else labels)\n",
    "        def __getitem__(self, idx):\n",
    "            item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "            item[\"labels\"] = self.labels[idx]\n",
    "            return item\n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "\n",
    "    train_ds = NewsDataset(train_enc, train_labels)\n",
    "    val_ds   = NewsDataset(val_enc,   val_labels)\n",
    "\n",
    "    # Load fresh model for each fold\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "    # Training args (you can adjust per your GPU/memory)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results/fold{fold}\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=0,\n",
    "        disable_tqdm=False,\n",
    "    )\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # Train & eval\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "\n",
    "    # Record F1\n",
    "    fold_f1 = metrics[\"eval_f1\"]\n",
    "    fold_results.append(fold_f1)\n",
    "    print(f\"Fold {fold} F1 = {fold_f1:.4f}\")\n",
    "\n",
    "# 6. Summarize\n",
    "avg_f1 = sum(fold_results) / len(fold_results)\n",
    "print(f\"\\nCross-validated F1 scores: {fold_results}\")\n",
    "print(f\"Mean F1: {avg_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f91e0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_1057128/2215723649.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  final_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.444800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./final_model/tokenizer_config.json',\n",
       " './final_model/special_tokens_map.json',\n",
       " './final_model/vocab.txt',\n",
       " './final_model/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize entire corpus\n",
    "full_enc = tokenize_texts(df[\"text\"])\n",
    "full_ds  = NewsDataset(full_enc, df[\"label_enc\"])\n",
    "\n",
    "# Load a fresh model\n",
    "final_model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "\n",
    "# (Optional) freeze layers if you found that helpful\n",
    "for i, layer in enumerate(final_model.bert.encoder.layer):\n",
    "    if i < 8:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "# TrainingArguments for the “final” run\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"./final_model\",\n",
    "    num_train_epochs=3,                # train a bit longer now\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=3e-5,                # or whatever your best CV LR was\n",
    "    weight_decay=0.01,                 # from your inner CV\n",
    "    logging_steps=50,\n",
    "    save_steps=500,\n",
    ")\n",
    "\n",
    "# Trainer & train\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=final_args,\n",
    "    train_dataset=full_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,   # so you can still call evaluate()\n",
    ")\n",
    "final_trainer.train()\n",
    "\n",
    "# Save model & tokenizer\n",
    "final_trainer.save_model(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fc5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example 1 ---\n",
      "Born a few weeks into the Gaza war, infant twins Wesam and Naeem Abu Anza were buried on Sunday, the youngest of 14 members of the same family whom Gaza health authorities say were killed in an Israel …\n",
      "Predicted = 1  (conf=0.89)\n",
      "\n",
      "--- Example 2 ---\n",
      "Volodymyr Zelensky  has hailed the UK’s £2.5 billion military aid package for  Ukraine , as  Rishi Sunak  promised to continue to stand with the country in its fight against Russia. The  Prime Ministe …\n",
      "Predicted = 0  (conf=0.94)\n",
      "\n",
      "--- Example 3 ---\n",
      "Dr. Siegel: I've changed my view of Fetterman and now see his 'personal courage'   Fox News medical contributor Dr. Marc Siegel applauds Democratic Sen. John Fetterman's recent candid interview on his …\n",
      "Predicted = 0  (conf=0.97)\n",
      "\n",
      "--- Example 4 ---\n",
      "Dozens of global traditions could be part of UNESCO's intangible global heritage list this week. These include Italian opera singing and Bangladeshi rickshaw art as well as the Peruvian delicacy of ce …\n",
      "Predicted = 0  (conf=0.95)\n",
      "\n",
      "--- Example 5 ---\n",
      "After 22 farcical days, thirteen failed candidates, four nominees and four floor votes, the Republicans have finally elected a House speaker. Louisiana 's Mike Johnson has been unanimously selected by …\n",
      "Predicted = 0  (conf=0.97)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the final model back (just to demo inference)\n",
    "from transformers import BertForSequenceClassification\n",
    "inference_model = BertForSequenceClassification.from_pretrained(\"./final_model\")\n",
    "inference_model.eval()\n",
    "\n",
    "# Pick 5 random articles\n",
    "sample = df.sample(5, random_state=123).reset_index(drop=True)\n",
    "texts  = sample[\"text\"].tolist()\n",
    "\n",
    "# Tokenize & to device\n",
    "enc = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    logits = inference_model(enc[\"input_ids\"], attention_mask=enc[\"attention_mask\"]).logits\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "# Map back to label names\n",
    "inv_map = {i: cls for i, cls in enumerate(le.classes_)}\n",
    "\n",
    "for i, txt in enumerate(texts):\n",
    "    pred_id   = torch.argmax(probs[i]).item()\n",
    "    confidence= probs[i, pred_id].item()\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(txt[:200].replace(\"\\n\",\" \"), \"…\")\n",
    "    print(f\"Predicted = {inv_map[pred_id]}  (conf={confidence:.2f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b5f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Article 1 ---\n",
      "Title:  ‘Kamala Harris, Joe Biden ignored Hindus’: Donald Trump condemns violence against minorities\n",
      "Predicted: irrelevant  (confidence: 0.97)\n",
      "\n",
      "--- Article 2 ---\n",
      "Title:  Israeli firefighting teams battle bushfires near Jerusalem for second day\n",
      "Predicted: relevant  (confidence: 0.80)\n",
      "\n",
      "--- Article 3 ---\n",
      "Title:  NYC broker fee law — which protects tenants from costs — challenged by real estate agents in 11th-hour push\n",
      "Predicted: irrelevant  (confidence: 0.95)\n",
      "\n",
      "--- Article 4 ---\n",
      "Title:  US proposes 60-day ceasefire for Gaza; hostage-prisoner swap, plan shows\n",
      "Predicted: relevant  (confidence: 0.87)\n",
      "\n",
      "--- Article 5 ---\n",
      "Title:  Mother of Palestinian American boy slain in suburban Chicago hate crime testifies at trial\n",
      "Predicted: irrelevant  (confidence: 0.83)\n",
      "\n",
      "--- Article 6 ---\n",
      "Title:  5 things to know for Feb. 26: Air travel safety, Budget blueprint, Ukraine’s minerals, Immigration, Gaza ceasefire\n",
      "Predicted: irrelevant  (confidence: 0.97)\n",
      "\n",
      "--- Article 7 ---\n",
      "Title:  5 things to know for Feb. 27: Cabinet meeting, Foreign aid, Measles outbreak, Transgender troops, Middle East\n",
      "Predicted: irrelevant  (confidence: 0.96)\n",
      "\n",
      "--- Article 8 ---\n",
      "Title:  Pro-Palestinian student protesters clash with Barnard College staff during sit-in to protest student expulsions\n",
      "Predicted: irrelevant  (confidence: 0.95)\n",
      "\n",
      "--- Article 9 ---\n",
      "Title:  Palestinian homes were destroyed ‘for revenge,’ says Israeli soldier who served in Gaza\n",
      "Predicted: relevant  (confidence: 0.88)\n",
      "\n",
      "--- Article 10 ---\n",
      "Title:  Hamas leader Ismail Haniyeh was killed in Iran by bomb planted months before blast, source says\n",
      "Predicted: relevant  (confidence: 0.74)\n",
      "\n",
      "--- Article 11 ---\n",
      "Title:  Iran claims Hamas leader killed by ‘short-range projectile,’ contradicting reports it was hidden bomb\n",
      "Predicted: irrelevant  (confidence: 0.52)\n",
      "\n",
      "--- Article 12 ---\n",
      "Title:  Israeli airstrikes on two Gaza shelters kill 17, says Gaza Civil Defense\n",
      "Predicted: relevant  (confidence: 0.85)\n",
      "\n",
      "--- Article 13 ---\n",
      "Title:  Anti-Netanyahu protests erupt in Israel over delayed hostage deal\n",
      "Predicted: relevant  (confidence: 0.85)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import json5\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# 1) Paste in your JSON:\n",
    "with open(\"articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)   # <-- json.load on a file, not json.loads on a string\n",
    "\n",
    "texts    = [a[\"text\"] for a in articles]\n",
    "titles   = [a[\"title\"] for a in articles]\n",
    "\n",
    "# 3) Load your fine-tuned model & tokenizer\n",
    "MODEL_PATH = \"./final_model\"   # adjust if different\n",
    "tokenizer  = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model      = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "# 4) Tokenize inputs\n",
    "enc = tokenizer(\n",
    "    texts,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    max_length=256,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# 5) Run inference\n",
    "with torch.no_grad():\n",
    "    logits = model(\n",
    "        input_ids=enc[\"input_ids\"],\n",
    "        attention_mask=enc[\"attention_mask\"],\n",
    "    ).logits\n",
    "\n",
    "probs       = F.softmax(logits, dim=-1)\n",
    "pred_ids    = probs.argmax(dim=-1).tolist()\n",
    "confidences = probs.max(dim=-1).values.tolist()\n",
    "\n",
    "# 6) Map class IDs back to names\n",
    "label_map = {0: \"irrelevant\", 1: \"relevant\"}\n",
    "\n",
    "# 7) Print results\n",
    "for i, title in enumerate(titles):\n",
    "    print(f\"--- Article {i+1} ---\")\n",
    "    print(\"Title: \", title)\n",
    "    print(f\"Predicted: {label_map[pred_ids[i]]}  (confidence: {confidences[i]:.2f})\\n\")\n",
    "\n",
    "\n",
    "# High RECALL, precision not == 100% but close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4bff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Outer Fold 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6463, 'grad_norm': 8.24787712097168, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5087, 'grad_norm': 3.069215774536133, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.0999, 'train_samples_per_second': 34.754, 'train_steps_per_second': 4.59, 'train_loss': 0.5816445010049003, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5504566431045532, 'eval_accuracy': 0.7037037037037037, 'eval_f1': 0.0, 'eval_runtime': 0.2536, 'eval_samples_per_second': 212.959, 'eval_steps_per_second': 27.606, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.64, 'grad_norm': 2.9499199390411377, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5572, 'grad_norm': 9.155176162719727, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.5471, 'train_samples_per_second': 32.686, 'train_steps_per_second': 4.277, 'train_loss': 0.5644358992576599, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5360226631164551, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2502, 'eval_samples_per_second': 211.79, 'eval_steps_per_second': 27.972, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.626, 'grad_norm': 9.719393730163574, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.6005, 'grad_norm': 9.842891693115234, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.4518, 'train_samples_per_second': 39.253, 'train_steps_per_second': 5.136, 'train_loss': 0.608568583215986, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5433419942855835, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2539, 'eval_samples_per_second': 208.746, 'eval_steps_per_second': 27.57, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6638, 'grad_norm': 11.24498462677002, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4408, 'grad_norm': 4.921231746673584, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.8273, 'train_samples_per_second': 36.38, 'train_steps_per_second': 4.805, 'train_loss': 0.5519413096564156, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4736465811729431, 'eval_accuracy': 0.7037037037037037, 'eval_f1': 0.0, 'eval_runtime': 0.2604, 'eval_samples_per_second': 207.348, 'eval_steps_per_second': 26.878, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6327, 'grad_norm': 2.903373956680298, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5231, 'grad_norm': 9.467113494873047, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.8973, 'train_samples_per_second': 36.288, 'train_steps_per_second': 4.748, 'train_loss': 0.5357553788593837, 'epoch': 2.0}\n",
      "{'eval_loss': 0.47636184096336365, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2592, 'eval_samples_per_second': 204.444, 'eval_steps_per_second': 27.002, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6065, 'grad_norm': 10.479204177856445, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.57, 'grad_norm': 9.225403785705566, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.2315, 'train_samples_per_second': 29.593, 'train_steps_per_second': 3.872, 'train_loss': 0.5743180002485003, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5016201138496399, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2519, 'eval_samples_per_second': 210.369, 'eval_steps_per_second': 27.785, 'epoch': 2.0}\n",
      "Best inner F1 = 0.0000 with {'learning_rate': 2e-05, 'weight_decay': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.659, 'grad_norm': 5.981875896453857, 'learning_rate': 1.55e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5386, 'grad_norm': 4.960479736328125, 'learning_rate': 1.0500000000000001e-05, 'epoch': 1.0}\n",
      "{'loss': 0.4724, 'grad_norm': 3.3569984436035156, 'learning_rate': 5.500000000000001e-06, 'epoch': 1.5}\n",
      "{'loss': 0.5384, 'grad_norm': 10.607613563537598, 'learning_rate': 5.000000000000001e-07, 'epoch': 2.0}\n",
      "{'train_runtime': 8.7169, 'train_samples_per_second': 36.71, 'train_steps_per_second': 4.589, 'train_loss': 0.5520978927612304, 'epoch': 2.0}\n",
      "{'eval_loss': 0.47544392943382263, 'eval_accuracy': 0.7, 'eval_f1': 0.0, 'eval_runtime': 0.1914, 'eval_samples_per_second': 208.983, 'eval_steps_per_second': 26.123, 'epoch': 2.0}\n",
      "Outer Fold 1 F1 = 0.0000\n",
      "\n",
      "=== Outer Fold 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6564, 'grad_norm': 3.6161696910858154, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5293, 'grad_norm': 8.3313570022583, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 4.8453, 'train_samples_per_second': 43.754, 'train_steps_per_second': 5.779, 'train_loss': 0.5697400399616787, 'epoch': 2.0}\n",
      "{'eval_loss': 0.49598097801208496, 'eval_accuracy': 0.7037037037037037, 'eval_f1': 0.0, 'eval_runtime': 0.2541, 'eval_samples_per_second': 212.488, 'eval_steps_per_second': 27.545, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6381, 'grad_norm': 5.579145908355713, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.6484, 'grad_norm': 3.1049325466156006, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.9992, 'train_samples_per_second': 26.753, 'train_steps_per_second': 3.5, 'train_loss': 0.597965555531638, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5445151925086975, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2543, 'eval_samples_per_second': 208.444, 'eval_steps_per_second': 27.53, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6549, 'grad_norm': 6.612698554992676, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5735, 'grad_norm': 3.175189256668091, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 8.0195, 'train_samples_per_second': 26.685, 'train_steps_per_second': 3.491, 'train_loss': 0.584602849824088, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5416379570960999, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2574, 'eval_samples_per_second': 205.902, 'eval_steps_per_second': 27.195, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.638, 'grad_norm': 4.412699222564697, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4913, 'grad_norm': 8.525997161865234, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 8.1694, 'train_samples_per_second': 25.951, 'train_steps_per_second': 3.427, 'train_loss': 0.5249090365001133, 'epoch': 2.0}\n",
      "{'eval_loss': 0.41459861397743225, 'eval_accuracy': 0.7592592592592593, 'eval_f1': 0.3157894736842105, 'eval_runtime': 0.2587, 'eval_samples_per_second': 208.715, 'eval_steps_per_second': 27.056, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6165, 'grad_norm': 4.027401924133301, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.6501, 'grad_norm': 3.460874557495117, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 8.0075, 'train_samples_per_second': 26.725, 'train_steps_per_second': 3.497, 'train_loss': 0.5772701672145298, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5035780072212219, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2561, 'eval_samples_per_second': 206.957, 'eval_steps_per_second': 27.334, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6407, 'grad_norm': 7.96733283996582, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5445, 'grad_norm': 3.611819267272949, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.346, 'train_samples_per_second': 33.722, 'train_steps_per_second': 4.412, 'train_loss': 0.5542388728686741, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4935925304889679, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2565, 'eval_samples_per_second': 206.629, 'eval_steps_per_second': 27.291, 'epoch': 2.0}\n",
      "Best inner F1 = 0.1053 with {'learning_rate': 3e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6623, 'grad_norm': 7.379771709442139, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4552, 'grad_norm': 4.989931106567383, 'learning_rate': 1.575e-05, 'epoch': 1.0}\n",
      "{'loss': 0.375, 'grad_norm': 4.885223388671875, 'learning_rate': 8.25e-06, 'epoch': 1.5}\n",
      "{'loss': 0.3836, 'grad_norm': 4.521304607391357, 'learning_rate': 7.5e-07, 'epoch': 2.0}\n",
      "{'train_runtime': 8.1795, 'train_samples_per_second': 39.122, 'train_steps_per_second': 4.89, 'train_loss': 0.4690282344818115, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3691452145576477, 'eval_accuracy': 0.875, 'eval_f1': 0.782608695652174, 'eval_runtime': 0.1923, 'eval_samples_per_second': 207.97, 'eval_steps_per_second': 25.996, 'epoch': 2.0}\n",
      "Outer Fold 2 F1 = 0.7826\n",
      "\n",
      "=== Outer Fold 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6543, 'grad_norm': 5.2951507568359375, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5015, 'grad_norm': 4.484918594360352, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.7768, 'train_samples_per_second': 27.26, 'train_steps_per_second': 3.6, 'train_loss': 0.5172703521592277, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5110672116279602, 'eval_accuracy': 0.7407407407407407, 'eval_f1': 0.4166666666666667, 'eval_runtime': 0.2654, 'eval_samples_per_second': 203.435, 'eval_steps_per_second': 26.371, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6667, 'grad_norm': 10.417009353637695, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5071, 'grad_norm': 3.842071771621704, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.4576, 'train_samples_per_second': 33.139, 'train_steps_per_second': 4.336, 'train_loss': 0.5595980371747699, 'epoch': 2.0}\n",
      "{'eval_loss': 0.49762895703315735, 'eval_accuracy': 0.7547169811320755, 'eval_f1': 0.3157894736842105, 'eval_runtime': 0.2617, 'eval_samples_per_second': 202.531, 'eval_steps_per_second': 26.749, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6854, 'grad_norm': 2.89085054397583, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5174, 'grad_norm': 2.7104721069335938, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.5853, 'train_samples_per_second': 28.213, 'train_steps_per_second': 3.691, 'train_loss': 0.5893386432102748, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5274100303649902, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2581, 'eval_samples_per_second': 205.347, 'eval_steps_per_second': 27.121, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6185, 'grad_norm': 4.48584508895874, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4431, 'grad_norm': 9.795675277709961, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.7461, 'train_samples_per_second': 27.369, 'train_steps_per_second': 3.615, 'train_loss': 0.46119712931769236, 'epoch': 2.0}\n",
      "{'eval_loss': 0.45015376806259155, 'eval_accuracy': 0.7592592592592593, 'eval_f1': 0.5517241379310345, 'eval_runtime': 0.2633, 'eval_samples_per_second': 205.108, 'eval_steps_per_second': 26.588, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6468, 'grad_norm': 7.26979923248291, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4456, 'grad_norm': 11.053085327148438, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.9523, 'train_samples_per_second': 26.911, 'train_steps_per_second': 3.521, 'train_loss': 0.4873677151543753, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3919275104999542, 'eval_accuracy': 0.8301886792452831, 'eval_f1': 0.6896551724137931, 'eval_runtime': 0.261, 'eval_samples_per_second': 203.043, 'eval_steps_per_second': 26.817, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.671, 'grad_norm': 3.679316997528076, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4875, 'grad_norm': 3.104135751724243, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.9813, 'train_samples_per_second': 26.813, 'train_steps_per_second': 3.508, 'train_loss': 0.5627611875534058, 'epoch': 2.0}\n",
      "{'eval_loss': 0.47860270738601685, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2638, 'eval_samples_per_second': 200.877, 'eval_steps_per_second': 26.531, 'epoch': 2.0}\n",
      "Best inner F1 = 0.4138 with {'learning_rate': 3e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7156, 'grad_norm': 12.387845039367676, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4453, 'grad_norm': 8.460331916809082, 'learning_rate': 1.575e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3226, 'grad_norm': 7.382590293884277, 'learning_rate': 8.25e-06, 'epoch': 1.5}\n",
      "{'loss': 0.229, 'grad_norm': 13.870139122009277, 'learning_rate': 7.5e-07, 'epoch': 2.0}\n",
      "{'train_runtime': 9.4363, 'train_samples_per_second': 33.911, 'train_steps_per_second': 4.239, 'train_loss': 0.4281137466430664, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5037909746170044, 'eval_accuracy': 0.7, 'eval_f1': 0.5, 'eval_runtime': 0.1968, 'eval_samples_per_second': 203.256, 'eval_steps_per_second': 25.407, 'epoch': 2.0}\n",
      "Outer Fold 3 F1 = 0.5000\n",
      "\n",
      "=== Outer Fold 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6634, 'grad_norm': 4.5602803230285645, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5838, 'grad_norm': 5.724076271057129, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 8.1167, 'train_samples_per_second': 26.119, 'train_steps_per_second': 3.45, 'train_loss': 0.5877760819026402, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5209246873855591, 'eval_accuracy': 0.7037037037037037, 'eval_f1': 0.0, 'eval_runtime': 0.2642, 'eval_samples_per_second': 204.357, 'eval_steps_per_second': 26.491, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6637, 'grad_norm': 4.2112717628479, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5869, 'grad_norm': 6.754333972930908, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.0657, 'train_samples_per_second': 30.287, 'train_steps_per_second': 3.963, 'train_loss': 0.5691882457051959, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5358714461326599, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2527, 'eval_samples_per_second': 209.751, 'eval_steps_per_second': 27.703, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6632, 'grad_norm': 3.534224510192871, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.6175, 'grad_norm': 2.8870081901550293, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.467, 'train_samples_per_second': 33.091, 'train_steps_per_second': 4.33, 'train_loss': 0.6244322402136666, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5436270833015442, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2608, 'eval_samples_per_second': 203.194, 'eval_steps_per_second': 26.837, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6501, 'grad_norm': 4.405368328094482, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5439, 'grad_norm': 5.033812999725342, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.8986, 'train_samples_per_second': 35.941, 'train_steps_per_second': 4.747, 'train_loss': 0.5454211575644357, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4667733609676361, 'eval_accuracy': 0.7407407407407407, 'eval_f1': 0.2222222222222222, 'eval_runtime': 0.2588, 'eval_samples_per_second': 208.661, 'eval_steps_per_second': 27.049, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6529, 'grad_norm': 4.283801078796387, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.5417, 'grad_norm': 7.669592380523682, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.5228, 'train_samples_per_second': 32.808, 'train_steps_per_second': 4.293, 'train_loss': 0.531026576246534, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4680681526660919, 'eval_accuracy': 0.7169811320754716, 'eval_f1': 0.11764705882352941, 'eval_runtime': 0.2619, 'eval_samples_per_second': 202.335, 'eval_steps_per_second': 26.724, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6466, 'grad_norm': 2.964806318283081, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.6034, 'grad_norm': 3.215555429458618, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 6.5141, 'train_samples_per_second': 32.852, 'train_steps_per_second': 4.298, 'train_loss': 0.5988059384482247, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5048835277557373, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2556, 'eval_samples_per_second': 207.363, 'eval_steps_per_second': 27.388, 'epoch': 2.0}\n",
      "Best inner F1 = 0.1133 with {'learning_rate': 3e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6825, 'grad_norm': 10.350278854370117, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.5227, 'grad_norm': 8.166686058044434, 'learning_rate': 1.575e-05, 'epoch': 1.0}\n",
      "{'loss': 0.3841, 'grad_norm': 5.858389377593994, 'learning_rate': 8.25e-06, 'epoch': 1.5}\n",
      "{'loss': 0.3683, 'grad_norm': 4.441047668457031, 'learning_rate': 7.5e-07, 'epoch': 2.0}\n",
      "{'train_runtime': 7.8232, 'train_samples_per_second': 40.904, 'train_steps_per_second': 5.113, 'train_loss': 0.48939624428749084, 'epoch': 2.0}\n",
      "{'eval_loss': 0.3053387403488159, 'eval_accuracy': 0.85, 'eval_f1': 0.6666666666666666, 'eval_runtime': 0.1964, 'eval_samples_per_second': 203.681, 'eval_steps_per_second': 25.46, 'epoch': 2.0}\n",
      "Outer Fold 4 F1 = 0.6667\n",
      "\n",
      "=== Outer Fold 5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.644, 'grad_norm': 5.013405799865723, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4858, 'grad_norm': 9.479620933532715, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.7696, 'train_samples_per_second': 36.745, 'train_steps_per_second': 4.853, 'train_loss': 0.5699025392532349, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5228021144866943, 'eval_accuracy': 0.7037037037037037, 'eval_f1': 0.1111111111111111, 'eval_runtime': 0.2631, 'eval_samples_per_second': 205.214, 'eval_steps_per_second': 26.602, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6696, 'grad_norm': 3.5225231647491455, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4608, 'grad_norm': 4.820092678070068, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.2559, 'train_samples_per_second': 40.716, 'train_steps_per_second': 5.327, 'train_loss': 0.5358146173613412, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5210983157157898, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2611, 'eval_samples_per_second': 202.963, 'eval_steps_per_second': 26.806, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6204, 'grad_norm': 3.082446813583374, 'learning_rate': 1.3571428571428574e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.491, 'grad_norm': 3.537034749984741, 'learning_rate': 6.4285714285714295e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.6967, 'train_samples_per_second': 37.565, 'train_steps_per_second': 4.915, 'train_loss': 0.5546565907342094, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4838898479938507, 'eval_accuracy': 0.6981132075471698, 'eval_f1': 0.0, 'eval_runtime': 0.2616, 'eval_samples_per_second': 202.561, 'eval_steps_per_second': 26.753, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6397, 'grad_norm': 4.963001728057861, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4574, 'grad_norm': 9.165356636047363, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 5.0948, 'train_samples_per_second': 41.611, 'train_steps_per_second': 5.496, 'train_loss': 0.5377746820449829, 'epoch': 2.0}\n",
      "{'eval_loss': 0.4785335063934326, 'eval_accuracy': 0.7222222222222222, 'eval_f1': 0.21052631578947367, 'eval_runtime': 0.2601, 'eval_samples_per_second': 207.629, 'eval_steps_per_second': 26.915, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6515, 'grad_norm': 3.749399185180664, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4168, 'grad_norm': 4.520461082458496, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.0921, 'train_samples_per_second': 30.174, 'train_steps_per_second': 3.948, 'train_loss': 0.4892325145857675, 'epoch': 2.0}\n",
      "{'eval_loss': 0.47362592816352844, 'eval_accuracy': 0.7547169811320755, 'eval_f1': 0.3157894736842105, 'eval_runtime': 0.257, 'eval_samples_per_second': 206.233, 'eval_steps_per_second': 27.238, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.602, 'grad_norm': 2.950532913208008, 'learning_rate': 2.0357142857142858e-05, 'epoch': 0.7142857142857143}\n",
      "{'loss': 0.4514, 'grad_norm': 3.5660126209259033, 'learning_rate': 9.642857142857144e-06, 'epoch': 1.4285714285714286}\n",
      "{'train_runtime': 7.8665, 'train_samples_per_second': 27.204, 'train_steps_per_second': 3.559, 'train_loss': 0.5116022654942104, 'epoch': 2.0}\n",
      "{'eval_loss': 0.40703240036964417, 'eval_accuracy': 0.7924528301886793, 'eval_f1': 0.47619047619047616, 'eval_runtime': 0.2605, 'eval_samples_per_second': 203.459, 'eval_steps_per_second': 26.872, 'epoch': 2.0}\n",
      "Best inner F1 = 0.3342 with {'learning_rate': 3e-05, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6879, 'grad_norm': 11.426597595214844, 'learning_rate': 2.3250000000000003e-05, 'epoch': 0.5}\n",
      "{'loss': 0.4246, 'grad_norm': 7.648776531219482, 'learning_rate': 1.575e-05, 'epoch': 1.0}\n",
      "{'loss': 0.2924, 'grad_norm': 7.088068008422852, 'learning_rate': 8.25e-06, 'epoch': 1.5}\n",
      "{'loss': 0.3632, 'grad_norm': 5.967757225036621, 'learning_rate': 7.5e-07, 'epoch': 2.0}\n",
      "{'train_runtime': 9.9736, 'train_samples_per_second': 32.085, 'train_steps_per_second': 4.011, 'train_loss': 0.44202861189842224, 'epoch': 2.0}\n",
      "{'eval_loss': 0.452415406703949, 'eval_accuracy': 0.775, 'eval_f1': 0.5714285714285714, 'eval_runtime': 0.1986, 'eval_samples_per_second': 201.391, 'eval_steps_per_second': 25.174, 'epoch': 2.0}\n",
      "Outer Fold 5 F1 = 0.5714\n",
      "\n",
      "Nested CV F1 per fold: [0.0, 0.782608695652174, 0.5, 0.6666666666666666, 0.5714285714285714]\n",
      "Mean F1: 0.5041407867494824\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. Load & prepare data\n",
    "with open(\"training_articles.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw = json.load(f)\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"text\": item.get(\"text\", \"\"),\n",
    "        \"label\": 1 if str(item.get(\"relevant\", \"\")).lower() == \"true\" else 0,\n",
    "    }\n",
    "    for item in raw\n",
    "])\n",
    "\n",
    "# 2. Encode labels\n",
    "le = LabelEncoder()\n",
    "df[\"label_enc\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "# 3. Tokenizer helper\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_texts(texts, max_length=256):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "# 4. Dataset wrapper\n",
    "class NewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels    = torch.tensor(labels.values if hasattr(labels, \"values\") else labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = self.labels[idx]\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# 5. Metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(p.label_ids, preds),\n",
    "        \"f1\":       f1_score(p.label_ids, preds, average=\"binary\"),\n",
    "    }\n",
    "\n",
    "# 6. Hyperparameter grid for inner CV\n",
    "param_grid = [\n",
    "    {\"learning_rate\": 2e-5, \"weight_decay\": 0.0},\n",
    "    {\"learning_rate\": 3e-5, \"weight_decay\": 0.01},\n",
    "]\n",
    "\n",
    "outer_skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "outer_scores = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(\n",
    "    outer_skf.split(df[\"text\"], df[\"label_enc\"]), start=1\n",
    "):\n",
    "    print(f\"\\n=== Outer Fold {fold} ===\")\n",
    "    X_train, y_train = df[\"text\"].iloc[train_idx], df[\"label_enc\"].iloc[train_idx]\n",
    "    X_test,  y_test  = df[\"text\"].iloc[test_idx],  df[\"label_enc\"].iloc[test_idx]\n",
    "\n",
    "    # ---- Inner CV to select hyperparameters ----\n",
    "    best_inner_f1 = -1.0\n",
    "    best_params   = param_grid[0]  # fallback\n",
    "    inner_skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=fold)\n",
    "\n",
    "    for params in param_grid:\n",
    "        inner_f1s = []\n",
    "        for inner_tr, inner_val in inner_skf.split(X_train, y_train):\n",
    "            # Tokenize\n",
    "            enc_tr = tokenize_texts(X_train.iloc[inner_tr])\n",
    "            enc_val= tokenize_texts(X_train.iloc[inner_val])\n",
    "            ds_tr  = NewsDataset(enc_tr, y_train.iloc[inner_tr])\n",
    "            ds_val = NewsDataset(enc_val, y_train.iloc[inner_val])\n",
    "\n",
    "            # Load model & freeze first 8 layers\n",
    "            model = BertForSequenceClassification.from_pretrained(\n",
    "                \"bert-base-uncased\", num_labels=2\n",
    "            )\n",
    "            for i, layer in enumerate(model.bert.encoder.layer):\n",
    "                if i < 8:\n",
    "                    for p in layer.parameters():\n",
    "                        p.requires_grad = False\n",
    "\n",
    "            # Legacy TrainingArguments (no evaluation_strategy)\n",
    "            args = TrainingArguments(\n",
    "                output_dir=f\"./tmp/inner_fold{fold}\",\n",
    "                num_train_epochs=2,\n",
    "                per_device_train_batch_size=8,\n",
    "                per_device_eval_batch_size=8,\n",
    "                learning_rate=params[\"learning_rate\"],\n",
    "                weight_decay=params[\"weight_decay\"],\n",
    "                do_train=True,\n",
    "                do_eval=True,\n",
    "                logging_steps=10,\n",
    "                save_steps=0,\n",
    "                disable_tqdm=True,\n",
    "            )\n",
    "            trainer = Trainer(\n",
    "                model=model,\n",
    "                args=args,\n",
    "                train_dataset=ds_tr,\n",
    "                eval_dataset=ds_val,\n",
    "                compute_metrics=compute_metrics,\n",
    "            )\n",
    "            trainer.train()\n",
    "            metrics = trainer.evaluate()\n",
    "            inner_f1s.append(metrics[\"eval_f1\"])\n",
    "\n",
    "        avg_f1 = sum(inner_f1s) / len(inner_f1s)\n",
    "        if avg_f1 > best_inner_f1:\n",
    "            best_inner_f1, best_params = avg_f1, params\n",
    "\n",
    "    print(f\"Best inner F1 = {best_inner_f1:.4f} with {best_params}\")\n",
    "\n",
    "    # ---- Final training on full train, evaluation on outer test ----\n",
    "    enc_tr_full = tokenize_texts(X_train)\n",
    "    enc_te_full = tokenize_texts(X_test)\n",
    "    ds_tr_full  = NewsDataset(enc_tr_full, y_train)\n",
    "    ds_te_full  = NewsDataset(enc_te_full,  y_test)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=2\n",
    "    )\n",
    "    for i, layer in enumerate(model.bert.encoder.layer):\n",
    "        if i < 8:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"./results/fold{fold}\",\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        learning_rate=best_params[\"learning_rate\"],\n",
    "        weight_decay= best_params[\"weight_decay\"],\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        logging_steps=10,\n",
    "        save_steps=0,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds_tr_full,\n",
    "        eval_dataset=ds_te_full,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    final_metrics = trainer.evaluate()\n",
    "    outer_scores.append(final_metrics[\"eval_f1\"])\n",
    "    print(f\"Outer Fold {fold} F1 = {final_metrics['eval_f1']:.4f}\")\n",
    "\n",
    "# Summarize nested CV results\n",
    "print(\"\\nNested CV F1 per fold:\", outer_scores)\n",
    "print(\"Mean F1:\", sum(outer_scores) / len(outer_scores))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
