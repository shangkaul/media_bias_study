{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2799f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== abcnews.go.com (747 failures) ===\n",
      "   2025-05-31T20:00:04Z\n",
      "   2025-05-30T17:26:27Z\n",
      "   2025-05-30T10:29:59Z\n",
      "   2025-05-30T12:59:35Z\n",
      "   2025-05-29T01:33:41Z\n",
      "\n",
      "=== apnews.com (3584 failures) ===\n",
      "   2024-10-03T21:39:58\n",
      "   2025-05-19T09:07:26\n",
      "   2025-05-07T10:07:03\n",
      "   2025-05-02T22:35:51\n",
      "   2025-05-17T10:09:43\n",
      "\n",
      "=== cnbc.com (582 failures) ===\n",
      "   2023-10-07T16:23:27+0000\n",
      "   2023-10-08T07:48:54+0000\n",
      "   2023-10-07T06:57:36+0000\n",
      "   2023-10-08T18:39:35+0000\n",
      "   2023-10-08T16:09:31+0000\n",
      "\n",
      "=== cnn.com (2085 failures) ===\n",
      "   2025-05-21T06:52:00\n",
      "   2025-05-28T06:38:00\n",
      "   2025-05-06T16:14:00\n",
      "   2025-05-06T18:48:00\n",
      "   2025-05-09T11:00:00\n",
      "\n",
      "=== dailymail.co.uk (2990 failures) ===\n",
      "   00:53, 8 October 2023\n",
      "   00:51, 8 October 2023\n",
      "   00:00, 8 October 2023\n",
      "   22:53, 7 October 2023\n",
      "   22:22, 7 October 2023\n",
      "\n",
      "=== express.co.uk (1919 failures) ===\n",
      "   2023-10-31T22:47:00Z\n",
      "   2023-10-31T22:01:00Z\n",
      "   2023-10-31T21:48:00Z\n",
      "   2023-10-31T19:31:00Z\n",
      "   2023-10-31T19:04:00Z\n",
      "\n",
      "=== foxnews.com (2749 failures) ===\n",
      "    March 6, 2025 6:11pm EST\n",
      "    March 7, 2025 9:54am EST\n",
      "    March 8, 2025 10:05am EST\n",
      "    March 9, 2025 7:00am EDT\n",
      "    March 10, 2025 6:00am EDT\n",
      "\n",
      "=== hindustantimes.com (3889 failures) ===\n",
      "   2024-12-01T00:10:00\n",
      "   2025-05-01T09:59:00\n",
      "   2025-05-01T12:32:00\n",
      "   2025-05-01T16:04:00\n",
      "   2025-05-02T08:39:00\n",
      "\n",
      "=== independent.co.uk (6511 failures) ===\n",
      "   Friday 30 May 2025 23:13 BST\n",
      "   Saturday 31 May 2025 13:41 BST\n",
      "   Saturday 31 May 2025 18:04 BST\n",
      "   Saturday 31 May 2025 09:21 BST\n",
      "   Sunday 01 June 2025 02:02 BST\n",
      "\n",
      "=== india.com (539 failures) ===\n",
      "   2025-05-01T11:28:00\n",
      "   2025-05-01T23:39:00\n",
      "   2025-05-02T12:31:00\n",
      "   2025-05-05T17:00:00\n",
      "   2025-05-14T08:37:00\n",
      "\n",
      "=== indianexpress.com (2622 failures) ===\n",
      "   Updated: May 29, 2025  01:20 IST\n",
      "   Updated: May 31, 2025  02:01 IST\n",
      "   Updated: May 31, 2025  14:54 IST\n",
      "   Updated: May 31, 2025  22:22 IST\n",
      "   May 31, 2025 23:42 IST\n",
      "\n",
      "=== nbcnews.com (2 failures) ===\n",
      "   Wed Oct 18 2023 14:30:40 GMT+0000 (UTC)\n",
      "   Thu Oct 26 2023 14:27:00 GMT+0000 (UTC)\n",
      "\n",
      "=== news18.com (2886 failures) ===\n",
      "   May 31, 2025, 07:00 IST\n",
      "   May 31, 2025, 09:03 IST\n",
      "   May 31, 2025, 22:13 IST\n",
      "   May 29, 2025, 11:50 IST\n",
      "   May 29, 2025, 21:59 IST\n",
      "\n",
      "=== newsweek.com (2153 failures) ===\n",
      "   2023-10-03T06:00:01-04:00\n",
      "   2023-10-03T12:08:53-04:00\n",
      "   2023-10-05T06:00:01-04:00\n",
      "   2023-10-07T03:45:49-04:00\n",
      "   2023-10-07T04:40:21-04:00\n",
      "\n",
      "=== theguardian.com (496 failures) ===\n",
      "   Thu May 15 2025 16:03:48 GMT+0000 (Coordinated Universal Time)\n",
      "   Mon May 12 2025 20:15:47 GMT+0000 (Coordinated Universal Time)\n",
      "   Thu Apr 10 2025 14:11:32 GMT+0000 (Coordinated Universal Time)\n",
      "   Mon Apr 07 2025 14:22:03 GMT+0000 (Coordinated Universal Time)\n",
      "   Mon Mar 31 2025 16:57:57 GMT+0000 (Coordinated Universal Time)\n",
      "\n",
      "=== thesun.co.uk (207 failures) ===\n",
      "   2025-05-30T21:39:03+00:00\n",
      "   2025-05-28T13:43:57+00:00\n",
      "   2025-05-27T15:26:40+00:00\n",
      "   2025-05-25T09:49:52+00:00\n",
      "   2025-05-24T23:20:17+00:00\n",
      "\n",
      "=== usatoday.com (877 failures) ===\n",
      "   2025-05-30T09:07:48Z\n",
      "   2025-05-29T11:03:44Z\n",
      "   2025-05-29T10:00:42Z\n",
      "   2025-05-28T14:12:44Z\n",
      "   2025-05-27T09:33:21Z\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load & parse as before\n",
    "df = pd.read_parquet(\"data/filtered_articles.parquet\")\n",
    "df[\"parsed_date\"] = pd.to_datetime(df[\"date_published\"], errors=\"coerce\")\n",
    "\n",
    "# grab only the failures\n",
    "bad = df[df[\"parsed_date\"].isna() & df[\"date_published\"].notna()]\n",
    "\n",
    "# group by source and show up to 5 unique examples each\n",
    "for src, grp in bad.groupby(\"source_domain\"):\n",
    "    samples = grp[\"date_published\"].unique()[:5]\n",
    "    print(f\"\\n=== {src} ({len(grp)} failures) ===\")\n",
    "    for s in samples:\n",
    "        print(\"  \", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001e5f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dateparser\n",
    "\n",
    "# 1. Load your filtered articles\n",
    "df = pd.read_parquet(\"data/filtered_articles.parquet\")\n",
    "\n",
    "# 2. Define a parser that strips known prefixes and lets dateparser handle the rest\n",
    "def parse_date(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return pd.NaT\n",
    "    # Remove common “Updated:” prefix\n",
    "    s_clean = s.replace(\"Updated:\", \"\").strip()\n",
    "    # dateparser will handle ISO, GMT, “pm”/“am”, timezones, weekday names, etc.\n",
    "    dt = dateparser.parse(\n",
    "        s_clean,\n",
    "        settings={\n",
    "            \"RETURN_AS_TIMEZONE_AWARE\": False,\n",
    "            \"PREFER_DAY_OF_MONTH\": \"first\",\n",
    "            \"PARSERS\": [\"timestamp\", \"relative-time\", \"custom-formats\"]\n",
    "        }\n",
    "    )\n",
    "    return pd.to_datetime(dt) if dt else pd.NaT\n",
    "\n",
    "# 3. Apply it\n",
    "df[\"parsed_date\"]   = df[\"date_published\"].apply(parse_date)\n",
    "df[\"date_readable\"] = df[\"parsed_date\"].notna()\n",
    "\n",
    "# 4. Inspect the results\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles = (\"date_readable\",\"size\"),\n",
    "         readable_dates = (\"date_readable\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"]  = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]      = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "print(stats.sort_values(\"pct_readable\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046d628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dateparser\n",
    "\n",
    "# 1. Load your filtered articles\n",
    "df = pd.read_parquet(\"data/filtered_articles.parquet\")\n",
    "\n",
    "# 2. Define a robust parser\n",
    "def parse_date(s):\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return pd.NaT\n",
    "    s_clean = s.replace(\"Updated:\", \"\").strip()\n",
    "    dt = dateparser.parse(\n",
    "        s_clean,\n",
    "        settings={\"RETURN_AS_TIMEZONE_AWARE\": False}\n",
    "    )\n",
    "    return pd.to_datetime(dt) if dt else pd.NaT\n",
    "\n",
    "# 3. Apply to entire column\n",
    "df[\"publish_date_cleaned\"] = df[\"date_published\"].apply(parse_date)\n",
    "\n",
    "# 4. Flag readable vs unreadable\n",
    "df[\"date_readable\"] = df[\"publish_date_cleaned\"].notna()\n",
    "\n",
    "# 5. Compute per-source stats\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles   = (\"date_readable\",\"size\"),\n",
    "         readable_dates   = (\"date_readable\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"] = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]     = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "# 6. Print the stats\n",
    "print(stats.sort_values(\"pct_readable\"))\n",
    "\n",
    "# 7. Save the new Parquet\n",
    "df.to_parquet(\"data/filtered_articles_cleaned.parquet\", index=False)\n",
    "print(\"Saved cleaned data with publish_date_cleaned to data/filtered_articles_cleaned.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da1844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the cleaned articles DataFrame\n",
    "df = pd.read_parquet(\"data/filtered_articles_cleaned.parquet\")\n",
    "\n",
    "# 2. Function to extract YYYY/MM/DD from NYPost URLs\n",
    "def extract_nypost_date(url):\n",
    "    m = re.search(r\"https?://[^/]+/(\\d{4})/(\\d{2})/(\\d{2})/\", url)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "    y, mth, d = m.groups()\n",
    "    return pd.Timestamp(f\"{y}-{mth}-{d}\")\n",
    "\n",
    "# 3. Apply to NYPost rows\n",
    "mask = df[\"source_domain\"] == \"nypost.com\"\n",
    "df.loc[mask, \"publish_date_cleaned\"] = df.loc[mask, \"url\"].apply(extract_nypost_date)\n",
    "\n",
    "# 4. Update readability\n",
    "df[\"date_readable\"] = df[\"publish_date_cleaned\"].notna()\n",
    "\n",
    "# 5. Recompute stats\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles   = (\"date_readable\", \"size\"),\n",
    "         readable_dates   = (\"date_readable\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"] = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]     = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "# 6. Print the updated stats\n",
    "print(stats.sort_values(\"pct_readable\").to_string(index=False))\n",
    "\n",
    "# 7. Overwrite the cleaned Parquet\n",
    "df.to_parquet(\"data/filtered_articles_cleaned.parquet\", index=False)\n",
    "print(\"\\n✅ Saved updated DataFrame (with NYPost dates) to 'data/filtered_articles_cleaned.parquet'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load your cleaned articles DataFrame\n",
    "df = pd.read_parquet(\"data/filtered_articles_cleaned.parquet\")\n",
    "\n",
    "# 2. Month name → number map for Guardian URLs\n",
    "month_map = {\n",
    "    \"january\":   \"01\", \"february\": \"02\", \"march\":    \"03\",\n",
    "    \"april\":     \"04\", \"may\":      \"05\", \"june\":     \"06\",\n",
    "    \"july\":      \"07\", \"august\":   \"08\", \"september\":\"09\",\n",
    "    \"october\":   \"10\", \"november\": \"11\", \"december\": \"12\"\n",
    "}\n",
    "\n",
    "# 3. Function to extract date from Guardian URL\n",
    "def extract_guardian_date(url):\n",
    "    \"\"\"\n",
    "    Guardian URLs look like:\n",
    "      https://www.theguardian.com/<section>/<year>/<month-name>/<day>/...\n",
    "    This extracts year, month-name, day and returns a Timestamp.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"https?://[^/]+/[^/]+/(\\d{4})/([^/]+)/(\\d{1,2})/\", url)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "    year, mon_name, day = m.groups()\n",
    "    mon = month_map.get(mon_name.lower())\n",
    "    if not mon:\n",
    "        return pd.NaT\n",
    "    return pd.Timestamp(f\"{year}-{mon}-{int(day):02d}\")\n",
    "\n",
    "# 4. Apply only to Guardian rows\n",
    "mask = df[\"source_domain\"] == \"theguardian.com\"\n",
    "df.loc[mask, \"publish_date_cleaned\"] = df.loc[mask, \"url\"].apply(extract_guardian_date)\n",
    "\n",
    "# 5. Update readability flag\n",
    "df[\"date_readable\"] = df[\"publish_date_cleaned\"].notna()\n",
    "\n",
    "# 6. Recompute per-source stats\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles   = (\"date_readable\", \"size\"),\n",
    "         readable_dates   = (\"date_readable\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"] = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]     = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "# 7. Print the updated stats\n",
    "print(stats.sort_values(\"pct_readable\").to_string(index=False))\n",
    "\n",
    "# 8. Save back to Parquet\n",
    "df.to_parquet(\"data/filtered_articles_cleaned.parquet\", index=False)\n",
    "print(\"\\n✅ Updated 'publish_date_cleaned' for theguardian.com and saved to data/filtered_articles_cleaned.parquet.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8624d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load your already‐cleaned DataFrame\n",
    "df = pd.read_parquet(\"data/filtered_articles_cleaned.parquet\")\n",
    "\n",
    "# 2. Flexible regex to find the first 4-digit segment → month → day\n",
    "guardian_re = re.compile(\n",
    "    r\"https?://[^/]+/(?:[^/]+/)*?(\\d{4})/([^/]+)/(\\d{1,2})/\"\n",
    ")\n",
    "\n",
    "def extract_guardian_from_url(url):\n",
    "    m = guardian_re.search(url)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "    year, mon_name, day = m.groups()\n",
    "    # month as two digits: Jan→01, Feb→02, … May→05, etc.\n",
    "    try:\n",
    "        month = f\"{pd.to_datetime(mon_name[:3], format='%b').month:02d}\"\n",
    "    except:\n",
    "        return pd.NaT\n",
    "    return pd.Timestamp(f\"{year}-{month}-{int(day):02d}\")\n",
    "\n",
    "# 3. Only re‐extract for guardian rows still unreadable:\n",
    "mask = (df[\"source_domain\"] == \"theguardian.com\") & df[\"publish_date_cleaned\"].isna()\n",
    "df.loc[mask, \"publish_date_cleaned\"] = (\n",
    "    df.loc[mask, \"url\"]\n",
    "      .apply(extract_guardian_from_url)\n",
    ")\n",
    "\n",
    "# 4. Update the readability flag\n",
    "df[\"date_readable\"] = df[\"publish_date_cleaned\"].notna()\n",
    "\n",
    "# 5. Recompute & print per‐source stats\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles   = (\"date_readable\",\"size\"),\n",
    "         readable_dates   = (\"date_readable\",\"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"] = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]     = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "print(stats.sort_values(\"pct_readable\").to_string(index=False))\n",
    "\n",
    "# 6. Save back to Parquet\n",
    "df.to_parquet(\"data/filtered_articles_cleaned.parquet\", index=False)\n",
    "print(\"\\n✔️ Updated guardian dates and saved to data/filtered_articles_cleaned.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c1d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     source_domain  total_articles  readable_dates  unreadable_dates  pct_readable\n",
      " indianexpress.com            2818            2622               196     93.044713\n",
      "           bbc.com            2376            2253               123     94.823232\n",
      "      newsweek.com            2163            2153                10     99.537679\n",
      "    abcnews.go.com             747             747                 0    100.000000\n",
      "           cnn.com            2085            2085                 0    100.000000\n",
      "   dailymail.co.uk            2990            2990                 0    100.000000\n",
      "        apnews.com            3584            3584                 0    100.000000\n",
      "          cnbc.com             582             582                 0    100.000000\n",
      "       foxnews.com            2749            2749                 0    100.000000\n",
      "     express.co.uk            1919            1919                 0    100.000000\n",
      " independent.co.uk            6511            6511                 0    100.000000\n",
      "hindustantimes.com            3889            3889                 0    100.000000\n",
      "         india.com             539             539                 0    100.000000\n",
      "       nbcnews.com            1566            1566                 0    100.000000\n",
      "        news18.com            2886            2886                 0    100.000000\n",
      "        nypost.com            1877            1877                 0    100.000000\n",
      "   theguardian.com            3884            3884                 0    100.000000\n",
      "      thesun.co.uk             207             207                 0    100.000000\n",
      "      usatoday.com             877             877                 0    100.000000\n",
      "\n",
      "✅ CNN dates extracted from URLs and saved to data/filtered_articles_cleaned.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 1. Load the cleaned articles DataFrame\n",
    "df = pd.read_parquet(\"data/filtered_articles_cleaned.parquet\")\n",
    "\n",
    "# 2. Function to extract YYYY/MM/DD from CNN URLs\n",
    "def extract_cnn_date(url):\n",
    "    \"\"\"\n",
    "    CNN URLs typically start with:\n",
    "       https://www.cnn.com/YYYY/MM/DD/...\n",
    "    This regex captures the year, month, and day segments.\n",
    "    \"\"\"\n",
    "    m = re.search(r\"https?://(?:www\\.)?cnn\\.com/(\\d{4})/(\\d{2})/(\\d{2})/\", url)\n",
    "    if not m:\n",
    "        return pd.NaT\n",
    "    year, month, day = m.groups()\n",
    "    return pd.Timestamp(f\"{year}-{month}-{day}\")\n",
    "\n",
    "# 3. Apply to CNN rows still missing a cleaned date\n",
    "mask = (df[\"source_domain\"] == \"cnn.com\") & df[\"publish_date_cleaned\"].isna()\n",
    "df.loc[mask, \"publish_date_cleaned\"] = (\n",
    "    df.loc[mask, \"url\"]\n",
    "      .apply(extract_cnn_date)\n",
    ")\n",
    "\n",
    "# 4. Update the readability flag\n",
    "df[\"date_readable\"] = df[\"publish_date_cleaned\"].notna()\n",
    "\n",
    "# 5. Recompute per-source stats\n",
    "stats = (\n",
    "    df.groupby(\"source_domain\")\n",
    "      .agg(\n",
    "         total_articles = (\"date_readable\", \"size\"),\n",
    "         readable_dates = (\"date_readable\", \"sum\")\n",
    "      )\n",
    "      .reset_index()\n",
    ")\n",
    "stats[\"unreadable_dates\"] = stats[\"total_articles\"] - stats[\"readable_dates\"]\n",
    "stats[\"pct_readable\"]     = 100 * stats[\"readable_dates\"] / stats[\"total_articles\"]\n",
    "\n",
    "# 6. Print the updated stats\n",
    "print(stats.sort_values(\"pct_readable\").to_string(index=False))\n",
    "\n",
    "# 7. Save your updates back into the same Parquet\n",
    "df.to_parquet(\"data/filtered_articles_cleaned.parquet\", index=False)\n",
    "print(\"\\n✅ CNN dates extracted from URLs and saved to data/filtered_articles_cleaned.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c4967",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
